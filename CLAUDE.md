# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Recommended Command (Best Quality - Multi-Constraint Optimization + Joint Angles)

```bash
uv run python main.py \
  --video data/input/joey.mp4 \
  --height 1.78 \
  --mass 75 \
  --age 30 \
  --sex male \
  --anatomical-constraints \
  --bone-length-constraints \
  --estimate-missing \
  --force-complete \
  --augmentation-cycles 20 \
  --multi-constraint-optimization \
  --compute-all-joint-angles \
  --plot-all-joint-angles
```

**Results** (tested on joey.mp4, 535 frames, 20 cycles):
- ‚è±Ô∏è Processing Time: **~45 seconds** (with GPU acceleration)
- üìè Bone Length Improvement: **68.0%** (0.113 ‚Üí 0.036 CV)
- üéØ Marker Quality: **59/65 markers** (4 unreliable markers auto-filtered)
- üìä Joint Angles: **12 joint groups** computed (pelvis, hip, knee, ankle, trunk, shoulder, elbow)
- ‚úì No scattered markers - unreliable augmented markers filtered by temporal variance
- ‚úì Stable medial markers - distance constraints prevent optimization-induced noise
- ‚úì Organized output - automatic cleanup into clean directory structure
- ‚úì GPU acceleration - 3-10x speedup on augmentation (automatic CPU fallback if unavailable)

## Joint Angle Computation (ISB-Compliant)

### Comprehensive Joint Angles (ALL Joints - RECOMMENDED)

Compute **ALL** joint angles (pelvis, lower body, trunk, upper body) using ISB-compliant anatomical coordinate systems:

```bash
# Comprehensive: ALL joints (pelvis + lower + trunk + upper body)
uv run python main.py \
  --video data/input/joey.mp4 \
  --height 1.78 --mass 75 --age 30 --sex male \
  --anatomical-constraints --bone-length-constraints \
  --estimate-missing --force-complete \
  --augmentation-cycles 20 \
  --multi-constraint-optimization \
  --compute-all-joint-angles \
  --plot-all-joint-angles \
  --save-angle-comparison
```

**Outputs** (automatically organized in `joint_angles/` subdirectory):
- `joint_angles/<video>_angles_pelvis.csv` - Pelvis global orientation (tilt, obliquity, rotation)
- `joint_angles/<video>_angles_hip_{R|L}.csv` - Hip 3-DOF angles (flexion, abduction, rotation)
- `joint_angles/<video>_angles_knee_{R|L}.csv` - Knee 3-DOF angles (flexion, varus/valgus, tibial rotation)
- `joint_angles/<video>_angles_ankle_{R|L}.csv` - Ankle 3-DOF angles (dorsi/plantar, inversion/eversion, rotation)
- `joint_angles/<video>_angles_trunk.csv` - Trunk 3-DOF angles (flexion, lateral flexion, rotation)
- `joint_angles/<video>_angles_shoulder_{R|L}.csv` - Shoulder 3-DOF angles (exo/endo, flexion, abduction)
- `joint_angles/<video>_angles_elbow_{R|L}.csv` - Elbow flexion angle
- `joint_angles/<video>_all_joint_angles.png` - Comprehensive multi-panel visualization (ALL joints)
- `joint_angles/<video>_joint_angles_comparison.png` - Side-by-side comparison (right vs left, if --save-angle-comparison used)

**Joint Coverage**: 12 joint groups computed using ISB standards
- **Pelvis**: Global orientation relative to world frame
- **Lower Body**: Hip, Knee, Ankle (both sides, 3-DOF each)
- **Trunk**: Thorax relative to pelvis
- **Upper Body**: Shoulder (3-DOF), Elbow (1-DOF), both sides

**Important**: Requires `--force-complete` to estimate Hip Joint Centers (RHJC/LHJC) and shoulder clusters.

### Individual Joint Angle Computation

For specific joints only:

```bash
# Lower limb only (hip, knee, ankle - one side)
uv run python main.py --video data/input/joey.mp4 --force-complete --compute-joint-angles --plot-joint-angles --joint-angle-side R

# Upper body only (trunk, shoulder, elbow - one side)
uv run python main.py --video data/input/joey.mp4 --force-complete --compute-upper-body-angles --plot-upper-body-angles --upper-body-side R
```

**Outputs** (individual):
- `<video>_angles_{R|L}.csv` - Lower limb angles (9 DOF: 3 per joint)
- `<video>_upper_angles_{R|L}.csv` - Upper body angles (7 DOF: trunk + shoulder + elbow)

## Neural Depth Refinement (Advanced/Optional)

For research applications requiring maximum depth accuracy, the pipeline includes a **PoseFormer-based neural depth refinement** model trained on CMU Motion Capture data.

### Training the Model

```bash
# 1. Install neural dependencies (PyTorch, CUDA, etc.)
uv sync --group neural

# 2. Generate training data from CMU mocap (208K sequences)
uv run --group neural python training/generate_training_data.py

# 3. Train PoseFormer model (~9-10 hours on RTX 5080)
uv run --group neural python scripts/train_depth_model.py \
  --batch-size 64 \
  --epochs 50 \
  --workers 8 \
  --fp16
```

**Training details:**
- **Dataset**: 208,440 temporal sequences from CMU Motion Capture
- **Ground truth**: Professional mocap data (converted from BVH with forward kinematics)
- **Simulation**: Depth noise added to Z-axis (30mm, 50mm, 80mm) at 6 camera angles (0-75¬∞)
- **Architecture**: PoseFormer (25.5M parameters)
  - Transformer-based with 6 layers, 8 attention heads
  - Temporal context: 11-frame sliding windows
  - Input features: x, y, z, visibility, variance, is_augmented, marker_type, camera_angle
  - Output: Per-marker depth corrections (delta_z) + confidence scores
- **Loss function**: Biomechanical constraints (bone length, ground plane, symmetry, smoothness, joint angles)
- **Training time**: ~9-10 hours (50 epochs, RTX 5080)
- **Output**: `models/checkpoints/best_model.pth`

### Applying Depth Refinement

```bash
# Apply trained model to refine TRC depth
uv run --group neural python scripts/apply_depth_refinement.py \
  --input data/output/pose-3d/joey/joey_final.trc \
  --model models/checkpoints/best_model.pth \
  --output data/output/pose-3d/joey/joey_refined.trc
```

**How it works:**
1. **Camera angle estimation**: Automatically calculates viewing angle from torso plane orientation
   - Uses shoulder-hip cross product to compute torso normal vector
   - Angle = deviation from camera direction (+Z axis)
   - Example: 0¬∞ frontal, 45¬∞ angled, 90¬∞ profile
2. **Temporal windowing**: Processes each frame with 11-frame context (¬±5 frames)
3. **Neural inference**: PoseFormer predicts depth corrections using biomechanical knowledge
4. **Output**: Refined TRC with corrected Z-coordinates

**When to use:**
- Research requiring high depth accuracy (inverse dynamics, force estimation)
- Motion capture validation and quality assessment
- Comparative biomechanics studies
- **NOT needed** for standard joint angle computation (multi-constraint optimization is sufficient)

**Data requirements:**
- CMU Motion Capture dataset (~2GB): `git clone https://github.com/una-dinosauria/cmu-mocap.git data/training/cmu_mocap/`
- Training generates ~500MB of NPZ files in `data/training/cmu_converted/`

## Project Overview

HumanPose is a 3D human pose estimation pipeline that uses MediaPipe for landmark detection and Pose2Sim for marker augmentation. The pipeline processes video input through multiple stages: capture/detection, CSV export, TRC conversion, and augmentation to produce biomechanics-compatible output files.

## Build and Development Commands

### Environment Setup
- `uv sync` - Install Python 3.12 toolchain and dependencies from pyproject.toml/uv.lock

### GPU Acceleration (Optional)

The pipeline automatically uses GPU acceleration for Pose2Sim LSTM inference when available, providing **3-10x speedup** on multi-cycle augmentation.

**Requirements:**
- NVIDIA GPU with CUDA support
- CUDA Toolkit 12.x
- cuDNN 9 for CUDA 12

**Installation (Ubuntu/WSL2):**
```bash
# Add NVIDIA CUDA repository
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update

# Install CUDA Toolkit and cuDNN
sudo apt install -y cuda-toolkit-12-6 libcudnn9-cuda-12

# Reinstall onnxruntime-gpu to detect new CUDA libraries
uv pip uninstall onnxruntime-gpu
uv pip install onnxruntime-gpu --force-reinstall
```

**Verify GPU is available:**
```bash
uv run python -c "import onnxruntime as ort; print('Available providers:', ort.get_available_providers())"
```

You should see `CUDAExecutionProvider` in the output.

**Automatic Fallback:**
- If GPU is not available, the pipeline **automatically falls back to CPU** with no user intervention
- CPU-only systems work without any code changes
- The GPU patch gracefully degrades: `[GPU] Warning: CUDA provider not available, using CPU`

**Implementation:**
- `src/markeraugmentation/gpu_config.py` - GPU configuration module
- `main.py` calls `patch_pose2sim_gpu()` at startup to enable CUDA acceleration
- ONNX Runtime provider order: `CUDA ‚Üí CPU` (automatic fallback)

### Running the Pipeline
```bash
uv run python main.py \
  --video data/input/<name>.mp4 \
  --height <meters> \
  --mass <kg> \
  --age <years> \
  --sex <male|female>
```

Outputs are written to `data/output/pose-3d/<basename>/`

**Note**: The pipeline now defaults to 20 augmentation cycles with 0.3 visibility threshold for best quality/speed balance. Adjust with `--augmentation-cycles N` as needed.

### Output Organization

The pipeline automatically organizes outputs into a clean directory structure:

```
data/output/pose-3d/<video_name>/
‚îú‚îÄ‚îÄ <video>_final.trc               # Final optimized skeleton (59-65 markers, some filtered)
‚îú‚îÄ‚îÄ <video>_initial.trc             # Initial TRC from MediaPipe (22 markers)
‚îú‚îÄ‚îÄ <video>_raw_landmarks.csv       # Raw MediaPipe landmarks
‚îî‚îÄ‚îÄ joint_angles/                   # Joint angle analysis (if computed)
    ‚îú‚îÄ‚îÄ <video>_all_joint_angles.png         # Comprehensive visualization
    ‚îú‚îÄ‚îÄ <video>_angles_pelvis.csv            # Pelvis angles
    ‚îú‚îÄ‚îÄ <video>_angles_hip_{R|L}.csv         # Hip angles (both sides)
    ‚îú‚îÄ‚îÄ <video>_angles_knee_{R|L}.csv        # Knee angles
    ‚îú‚îÄ‚îÄ <video>_angles_ankle_{R|L}.csv       # Ankle angles
    ‚îú‚îÄ‚îÄ <video>_angles_trunk.csv             # Trunk angles
    ‚îú‚îÄ‚îÄ <video>_angles_shoulder_{R|L}.csv    # Shoulder angles
    ‚îî‚îÄ‚îÄ <video>_angles_elbow_{R|L}.csv       # Elbow angles
```

**Automatic Cleanup**: Intermediate files (augmentation cycles, Config files, pose2sim projects) are automatically removed. See [OUTPUT_ORGANIZATION.md](docs/OUTPUT_ORGANIZATION.md) for details.

### Testing
- `uv run pytest` - Run all tests
- `uv run pytest -k <module_name>` - Run tests for specific module
- Test files mirror source structure under `tests/`

### Development Tools
- `uv run jupyter lab notebooks/` - Launch Jupyter notebooks with repo packages on path
- `uv run python -m black src tests` - Format code before committing

## Pipeline Architecture

The pipeline follows a 7-step orchestration model in `main.py`:

1. **Step 1 - Extraction** (`src/posedetector/`): MediaPipe Pose processes video frames and extracts world landmarks (33 points mapped to 22 Pose2Sim-aligned markers via `POSE_NAME_MAP`). Outputs CSV with columns: timestamp_s, landmark, x_m, y_m, z_m, visibility.

2. **Step 2 - TRC Conversion** (`src/datastream/`): Converts CSV to TRC format using `ORDER_22` marker ordering. Derives synthetic markers (Hip, Neck) from parent landmarks (Hip = mean(LHip, RHip), Neck = mean(LShoulder, RShoulder)) per `DERIVED_PARENTS`. Missing landmarks remain empty - no placeholders or duplicates.

3. **Step 3 - Augmentation** (`src/markeraugmentation/`): Invokes Pose2Sim's `augment_markers_all` multiple times (default 20 cycles) via CLI or Python shim to add full OpenCap marker set using LSTM prediction. Multi-cycle averaging with small perturbations (1mm Gaussian noise per cycle) significantly improves marker completion rates. This adds **43 additional anatomical markers** (shoulder clusters, thigh clusters, medial/lateral joint markers, hip joint centers) to the original 22 markers, creating a 65-marker dataset. Intermediate cycle files are automatically cleaned up.

4. **Step 3.5 - Force Complete** (optional, `--force-complete`): Estimates shoulder clusters and hip joint centers using Bell et al. 1990 regression. Required for joint angle computation.

5. **Step 4 - Multi-Constraint Optimization** (optional, `--multi-constraint-optimization`): Applies biomechanical constraints in 3 phases: (0) Filter unreliable augmented markers, (1) Stabilize bone lengths, (2) Apply ground plane and hip width constraints. Results in 59-65 markers (4-6 typically filtered).

6. **Step 5 - Joint Angles** (optional, `--compute-all-joint-angles`): Computes ISB-compliant joint angles for all 12 joint groups. Automatically organized into `joint_angles/` subdirectory.

7. **Step 6 - Cleanup & Organization**: Automatically renames files (`<basename>_final.trc`, `<basename>_initial.trc`, `<basename>_raw_landmarks.csv`), organizes joint angles into subdirectory, and removes intermediate files.

## Key Implementation Details

### Strict Mode Rules
- All landmark processing follows "strict mode": no placeholder values, no duplicate markers, explicit visibility thresholds (default 0.3)
- Derived markers require both parents present; missing data stays blank
- CSV rows are sorted by (timestamp, landmark) for deterministic output
- Each run appends to `docs/BUILD_LOG.md` via `append_build_log()`

### Marker Estimation Pipeline
**Problem**: MediaPipe may miss landmarks (right arm occluded, head out of frame, etc.), causing Pose2Sim augmentation to fail completely.

**Solution**: Two-stage estimation approach:

1. **Pre-augmentation** (`--estimate-missing`): Fills missing INPUT markers before Pose2Sim
   - Mirrors missing right arm from left arm (using body symmetry)
   - Extrapolates Head from Nose and Neck vectors
   - Estimates SmallToes from BigToe-Heel geometry
   - Implemented in `src/datastream/marker_estimation.py`
   - Significantly improves LSTM augmentation success (0% ‚Üí 81% with incomplete data)

2. **Post-augmentation** (`--force-complete`): Fills missing OUTPUT markers after Pose2Sim
   - Estimates shoulder clusters (r_sh1-3, L_sh1-3) from shoulder-elbow vectors
   - Calculates hip joint centers (RHJC, LHJC) using Bell et al. 1990 regression
   - Implemented in `src/datastream/post_augmentation_estimation.py`
   - Targets the 8 markers LSTM often skips (81% ‚Üí 100%)

**Recommended**: Always use `--estimate-missing` for incomplete pose detection. Use `--force-complete` when you need all 43 augmented markers for biomechanical analysis.

### Module Responsibilities
- **`mediastream/`** - Video I/O using OpenCV, returns RGB frames + FPS
- **`posedetector/`** - MediaPipe inference, maps 33 landmarks ‚Üí 22 markers via `POSE_NAME_MAP`
- **`datastream/`** - CSV/TRC writers, marker estimation:
  - Implements ORDER_22, DERIVED_PARENTS logic for TRC conversion
  - `marker_estimation.py` - Pre-augmentation estimation using anatomical symmetry
  - `post_augmentation_estimation.py` - Post-augmentation completion for shoulder clusters and HJC
- **`anatomical/`** - Anatomical constraints and corrections:
  - `anatomical_constraints.py` - Bone length smoothing, pelvis depth filtering, ground plane detection
  - `bone_length_constraints.py` - Temporal bone length consistency enforcement to reduce depth noise
  - `ground_plane_refinement.py` - Enhanced ground plane with stance detection and depth propagation
  - `joint_angle_depth_correction.py` - Joint angle constraint enforcement with depth adjustments
  - `multi_constraint_optimization.py` - Iterative multi-constraint optimization preventing cascading violations
- **`kinematics/`** - Joint angle computation and analysis using ISB standards:
  - `segment_coordinate_systems.py` - ISB-compliant anatomical coordinate systems (pelvis, femur, tibia, foot, trunk, humerus)
  - `angle_processing.py` - Unwrapping, smoothing, zeroing, clamping utilities for angle time series
  - `joint_angles_euler.py` - Lower limb angles (hip/knee/ankle) using Euler XYZ decomposition
  - `joint_angles_upper_body.py` - Upper body angles (trunk, shoulder, elbow) with XYZ and ZXY Euler
  - `comprehensive_joint_angles.py` - ALL joints (pelvis + lower + trunk + upper) in single unified call
  - `visualize_angles.py` - 3-panel time-series plots for individual joint groups
  - `visualize_comprehensive_angles.py` - Multi-panel grid plots for all 12 joint groups + side-by-side comparison
- **`markeraugmentation/`** - Pose2Sim integration with GPU acceleration:
  - Creates temp project structure, resolves CLI via POSE2SIM_CMD env var or local .venv/bin/pose2sim
  - `gpu_config.py` - Monkey-patches ONNX Runtime to use CUDA for 3-10x LSTM inference speedup
  - Automatic CPU fallback when GPU unavailable
- **`visualizedata/`** - 3D Matplotlib plotting for landmarks/TRC with auto-detection of marker sets:
  - Reads ALL markers from TRC data (not just header - critical for augmented files where header only lists 22 but data contains 65)
  - Auto-selects skeleton connections: `OPENCAP_CONNECTIONS` (33 connections) for 65-marker augmented data, `MEDIAPIPE_CONNECTIONS` for 22-33 marker data
  - OpenCap connections based on actual marker availability (not all 43 augmented markers may have data)
  - Uses anatomically correct connections: full limb chains (shoulder‚Üíelbow‚Üíwrist‚Üítoes), pelvis structure, augmented joint markers
  - Exports MP4 (ffmpeg) or GIF (Pillow) fallback
  - Interactive mode enabled by default (TkAgg backend)
- **`application/`** - Build logging and step scripts (mostly superseded by main.py orchestration)

### Data Flow
```
video ‚Üí frames (mediastream)
      ‚Üí landmarks (posedetector)
      ‚Üí CSV records (datastream)
      ‚Üí TRC (datastream)
      ‚Üí augmented TRC (markeraugmentation)
      ‚Üí [optimized TRC] (multi-constraint optimization)
      ‚Üí [joint angles] (kinematics) ‚Üí CSV + PNG in joint_angles/
      ‚Üí [organized output] (automatic cleanup)
```

### Important Flags
- `--show-video` - Renders MediaPipe preview window + exports `<name>_preview.mp4` (requires Qt/XCB + ffmpeg)
- `--plot-landmarks` - Displays extracted CSV landmarks in 3D Matplotlib viewer
- `--plot-augmented` - Visualizes augmented TRC and exports `<name>_LSTM_preview.mp4`
- `--visibility-min` - Threshold for landmark export (default 0.3 for better coverage)
- `--estimate-missing` - Estimates missing markers using anatomical symmetry before augmentation (recommended for incomplete poses)
- `--force-complete` - Post-processes augmented TRC to estimate shoulder clusters and hip joint centers (optional)
- `--augmentation-cycles` - Number of augmentation cycles to run and average (default 20 for best results)
- `--bone-length-constraints` - Enforces consistent bone lengths across frames to reduce depth noise (recommended for better accuracy)
- `--bone-length-tolerance` - Acceptable bone length deviation (default 0.15 = 15%)
- `--bone-depth-weight` - Weight for depth vs xy correction, 0-1 (default 0.8 focuses corrections on noisy z-axis)
- `--bone-length-report` - Print detailed statistics on bone length consistency improvements
- `--ground-plane-refinement` - Enhanced ground plane with stance detection and depth propagation up kinematic chains
- `--ground-contact-threshold` - Max distance from ground for foot contact detection (default 0.03m)
- `--min-contact-frames` - Minimum consecutive frames for valid stance phase (default 3)
- `--depth-propagation-weight` - Weight decay for depth correction propagation (default 0.7)
- `--compute-joint-angles` - Compute hip/knee/ankle joint angles using Euler decomposition (requires --force-complete)
- `--joint-angle-side` - Side for joint angle computation: R or L (default R)
- `--joint-angle-smooth-window` - Smoothing window for angle computation (default 9, 0=no smoothing)
- `--plot-joint-angles` - Generate 3-panel visualization of joint angles (Hip/Knee/Ankle)
- `--check-joint-constraints` - Check for biomechanical constraint violations in computed angles
- `--compute-upper-body-angles` - Compute trunk/shoulder/elbow angles (requires --force-complete)
- `--upper-body-side` - Side for upper body computation: R or L (default R)
- `--plot-upper-body-angles` - Visualize upper body angles (3-panel: trunk/shoulder/elbow)
- `--compute-all-joint-angles` - Compute ALL joints (pelvis + lower + trunk + upper) using ISB standards (RECOMMENDED)
- `--plot-all-joint-angles` - Generate comprehensive multi-panel visualization for all 12 joint groups
- `--save-angle-comparison` - Save side-by-side comparison plot (right vs left)
- `--multi-constraint-optimization` - Apply iterative multi-constraint optimization (RECOMMENDED for best quality)
- `--multi-constraint-iterations` - Max iterations for multi-constraint optimization (default 10)

## ORDER_22 Marker Set
The pipeline uses a fixed 22-marker layout aligned with Pose2Sim's OpenCap expectations:

Neck, RShoulder, LShoulder, RHip, LHip, RKnee, LKnee, RAnkle, LAnkle, RHeel, LHeel, RSmallToe, LSmallToe, RBigToe, LBigToe, RElbow, LElbow, RWrist, LWrist, Hip, Head, Nose

MediaPipe provides direct mappings for most landmarks (shoulders, elbows, wrists, hips, knees, ankles, heels, foot_index‚ÜíBigToe, nose). Neck and Hip are derived from shoulder/hip pairs. Some markers (SmallToe, Head) may remain empty depending on visibility.

## Testing Guidelines
- Use `tmp_path` fixtures for file I/O tests
- Prefer deterministic fixtures from `data/input/tests/` for integration tests
- Mock MediaPipe/OpenCV calls for heavy operations
- Golden CSVs/TRCs should be documented in PR descriptions
- Coverage priorities: parsing, error handling, visualization helpers

## Interactive Visualization

### Quick Start
```bash
uv run python visualize_interactive.py [path/to/file.trc]
```

If no path provided, defaults to `data/output/pose-3d/joey/joey_final.trc`

### Controls
- **Mouse drag**: Rotate 3D view
- **Slider**: Navigate through frames
- **Play/Pause button**: Animate playback
- **Close window**: Exit viewer

### Marker Count Detection
- Automatically detects 65-marker (augmented) vs 22-marker (non-augmented) data
- Uses `OPENCAP_CONNECTIONS` for augmented files with proper anatomical skeleton
- Uses `MEDIAPIPE_CONNECTIONS` for non-augmented files

## Multi-Constraint Optimization (RECOMMENDED)

The `--multi-constraint-optimization` flag enables iterative refinement of pose data through multiple biomechanical constraints, preventing the "cascading violation" problem where fixing one constraint breaks another.

### How It Works

The optimizer applies biomechanical constraints in 3 sequential phases:

**Phase 0: Augmented Marker Quality Filtering**
- Automatically filters unreliable augmented markers with high temporal variance (threshold: 0.05)
- Removes noisy LSTM predictions (typically 4-6 markers: heels, some medial markers) before optimization
- Preserves all MediaPipe markers (high confidence: 95%+)
- Typical result: 59/65 markers retained (4-6 noisy markers filtered)

**Phase 1: Bone Length Stabilization**
- Restores consistent segment lengths in main kinematic chain (hip‚Üíknee‚Üíankle)
- Uses distance constraints (not rigid movement) for augmented markers
- Medial knee/ankle markers maintain fixed distance from lateral parent markers
- Prevents optimization-induced noise in augmented markers
- Aggressive stabilization: 5 iterations, 8% tolerance, 95% depth weight

**Phase 2: Finalization**
- Ground plane alignment (feet above ground, threshold 0.03m)
- Hip width anthropometric constraint (0.20 √ó height ¬±15%)
- Heel Z-axis temporal smoothing for stability

**Key Innovation:** Augmented markers (medial knee/ankle, heels) use **distance constraints** instead of rigid kinematic chains. This prevents them from introducing noise during optimization while maintaining anatomical relationships.

### Results vs Standard Pipeline

| Metric | Standard | Multi-Constraint | Improvement |
|--------|----------|------------------|-------------|
| Bone length CV | 0.113 | 0.036 | **68.0%** |
| Markers with data | 65/65 | 59/65 | -4-6 unreliable filtered |
| Scattered markers | Common | None | **100%** |
| Ground violations | Variable | 0 | **100%** |
| Processing time | ~32s | ~45s | +13s |

### When to Use

- **RECOMMENDED** for all biomechanical analysis and motion capture workflows
- Automatically removes unreliable augmented markers (heels, noisy medial markers)
- Prevents scattered augmented markers during optimization
- Essential for accurate joint angle computation and inverse dynamics
- Use for any workflow requiring stable, biomechanically valid skeleton data

## Recommended Workflow

### For Best Quality (RECOMMENDED - Multi-Constraint Optimization + Joint Angles)
```bash
uv run python main.py \
  --video data/input/<video>.mp4 \
  --height <meters> \
  --mass <kg> \
  --age <years> \
  --sex <male|female> \
  --anatomical-constraints \
  --bone-length-constraints \
  --estimate-missing \
  --force-complete \
  --augmentation-cycles 20 \
  --multi-constraint-optimization \
  --compute-all-joint-angles \
  --plot-all-joint-angles
```

**Results** (tested on joey.mp4, 535 frames, 20 cycles):
- ‚è±Ô∏è Processing Time: **~45 seconds** (with GPU acceleration)
- üìè Bone Length Improvement: **68.0%** (0.113 ‚Üí 0.036 CV)
- üéØ Marker Quality: **59/65 markers** (4 unreliable auto-filtered)
- üìä Joint Angles: **12 joint groups** computed (pelvis, hip, knee, ankle, trunk, shoulder, elbow)
- ‚úì No scattered markers - unreliable augmented markers filtered by variance
- ‚úì Stable medial markers - distance constraints prevent noise
- ‚úì Organized output - automatic cleanup into clean directory structure
- ‚úì GPU acceleration - 3-10x speedup on augmentation (automatic CPU fallback)

**What it does**:
1. Extracts landmarks with MediaPipe (visibility threshold 0.3)
2. Estimates missing markers using anatomical symmetry
3. Applies pre-augmentation constraints (bone length, ground plane)
4. Runs 20 augmentation cycles with averaging for smooth markers
5. Estimates shoulder clusters and hip joint centers (`--force-complete`)
6. **Applies multi-constraint optimization** (3 phases):
   - **Phase 0**: Filters unreliable augmented markers (variance > 0.05)
   - **Phase 1**: Bone length stabilization with distance constraints for augmented markers
   - **Phase 2**: Ground plane alignment and hip width anthropometric constraints
7. **Computes comprehensive joint angles**: All 12 joint groups using ISB standards
8. **Automatic cleanup**: Organizes output into clean directory structure
9. Outputs: `<video>_final.trc`, `<video>_initial.trc`, `<video>_raw_landmarks.csv`, `joint_angles/` (13 files)

---

### For Standard Processing (No Multi-Constraint)
```bash
uv run python main.py \
  --video data/input/<video>.mp4 \
  --height <meters> \
  --mass <kg> \
  --age <years> \
  --sex <male|female> \
  --anatomical-constraints \
  --bone-length-constraints \
  --estimate-missing \
  --augmentation-cycles 20
```

**Performance**: ~32 seconds
**Use when**: Simple visualization or tracking tasks where joint angle accuracy isn't critical

### For Quick Testing (Single Cycle)
```bash
uv run python main.py \
  --video data/input/<video>.mp4 \
  --height 1.78 \
  --mass 75.0 \
  --age 30 \
  --sex male \
  --augmentation-cycles 1 \
  --estimate-missing
```

**Performance**: ~10-15 seconds

### For Visualization Only
```bash
uv run python main.py \
  --video data/input/<video>.mp4 \
  --height 1.78 \
  --mass 75.0 \
  --age 30 \
  --sex male \
  --plot-augmented
```

### For Quick Checks
```bash
# Check marker counts
python3 -c "
from pathlib import Path
import numpy as np
from src.visualizedata.visualize_data import VisualizeData
viz = VisualizeData()
_, frames = viz.load_trc_frames(Path('your_file.trc'))
frame = frames[0]
augmented = sum(1 for i in range(22, 65) if not np.isnan(frame[i]).any())
print(f'Augmented: {augmented}/43')
"
```

## Known Constraints & Troubleshooting

### Pose2Sim Augmentation Improvements
- **Multi-cycle averaging**: Default 20 cycles with small perturbations (0.5mm) and averaging for consistent results
- **Visibility threshold**: 0.3 default (lower = more data, more guessing allowed)
- **Typical results**: 100% completion of augmented markers that Pose2Sim generates (typically 35/43 markers with estimation)
- **Performance**: 20 cycles completes in ~30-45 seconds on modern hardware
- **Use `--estimate-missing`** for incomplete poses (occluded limbs, partial views)

### TRC File Structure Quirk
- Augmented TRC files have **header mismatch**: header lists 22 markers but data contains 65 markers
- This is normal Pose2Sim behavior - augmented markers are appended to data but not header
- `VisualizeData.load_trc_frames()` handles this by reading actual data columns, not header
- Custom TRC readers must count data columns (197 = 2 + 65*3) not header entries (68 = 2 + 22*3)

### Visualization
- **Interactive mode**: Default TkAgg backend opens GUI window (drag to rotate, slider to navigate frames)
- **Skeleton structure**: Clean connections - toe‚Üíankle‚Üíknee‚Üíhip‚Üíspine‚Üíshoulders, plus arms
- **Auto-detection**: Uses OpenCap connections for 50+ markers, MediaPipe connections for <50 markers
- **Headless mode**: Set `MPLBACKEND=Agg` env var for non-interactive rendering
- **MP4 export**: Requires ffmpeg; falls back to GIF (larger files, lower quality)
- **GUI features** (--show-video, --plot-*): Require Qt/XCB libs on Linux

### Common Issues
- **"No trc files found"**: Pose2Sim can't find input - check project_dir structure in Config.toml
- **Right arm missing**: Camera angle issue - MediaPipe can't detect occluded limbs; use `--estimate-missing` to mirror from left
- **Hip appears stuck**: Derived Hip marker averages LHip/RHip which dampens movement; use individual hip markers or RHJC/LHJC for analysis
- **Head marker very far**: Nose-Neck extrapolation unreliable when head tilted; Head marker often empty in MediaPipe output

### GPU Acceleration
- **Check GPU availability**: Run `uv run python -c "import onnxruntime as ort; print(ort.get_available_providers())"` - should see `CUDAExecutionProvider`
- **Missing CUDA libraries**: If GPU detected but not working, install CUDA Toolkit 12.x and cuDNN 9 (see GPU Acceleration section)
- **CPU fallback works automatically**: Pipeline prints `[GPU] Warning: CUDA provider not available, using CPU` and continues normally
- **Performance**: GPU provides 3-10x speedup on multi-cycle augmentation; CPU-only still works fine, just slower
- **Verify GPU usage**: Pipeline prints `[GPU] Pose2Sim GPU acceleration enabled (CUDA)` at startup when GPU is active
- **WSL2 GPU support**: Requires Windows 11 with WSL2 GPU passthrough enabled and NVIDIA drivers installed on Windows host

## Pelvis Angle Calculation (Validated)

The pelvis angle computation has been **validated against a reference implementation** in `use/scripts/compute_pelvis_global_angles.py`. The main codebase produces identical results (max diff < 0.001 deg).

### Pelvis Coordinate System (ISB-Compliant)

```python
# From ASIS/PSIS markers (augmented by Pose2Sim):
Z = normalize(RASIS - LASIS)       # Right (medial-lateral)
Y_temp = normalize(ASIS_mid - PSIS_mid)  # Up (superior)
X = normalize(Y_temp √ó Z)          # Forward (anterior)
Y = normalize(Z √ó X)               # Orthogonalized up
```

### Euler Angle Sequence
- **ZXY sequence** (clinical convention) via `scipy.spatial.transform.Rotation.as_euler('ZXY')`
- Returns: `[flex_Z, abd_X, rot_Y]` in degrees
- **Flex/Ext**: Rotation around Z (right) axis - sagittal plane tilt
- **Abd/Add**: Rotation around X (anterior) axis - frontal plane tilt
- **Rotation**: Rotation around Y (superior) axis - axial rotation

### Key Implementation Details
- **Smoothing**: Window size = 9 (applied to marker coordinates, not angles)
- **Zeroing**: `global_mean` mode (subtracts mean of entire trial)
- **No median filtering** for pelvis (only coordinate smoothing)
- **No clamping** for pelvis (global angles can exceed joint limits)
- **Continuity check**: Flips all axes if score < 0 to prevent 180¬∞ discontinuities

### Validation Data Location
```
use/output/
‚îú‚îÄ‚îÄ pose2sim_input_exact_LSTM_fixed.trc          # Reference TRC (65 markers, 709 frames)
‚îú‚îÄ‚îÄ pose2sim_input_exact_LSTM_fixed_pelvis_global_ZXY.csv  # Reference pelvis angles
‚îî‚îÄ‚îÄ pelvis_angles_plot.png                        # Reference visualization
```

### Running Validation
```bash
# Compare main codebase output with reference
uv run python scripts/compare_pelvis_output.py
```

**Expected output:**
```
pelvis_tilt_deg:      Max diff: 0.0005 deg  [PASS]
pelvis_obliquity_deg: Max diff: 0.0005 deg  [PASS]
pelvis_rotation_deg:  Max diff: 0.0005 deg  [PASS]
RESULT: ALL TESTS PASSED
```

### Critical Implementation Notes
1. **ASIS/PSIS markers required** - No fallback to Hip markers (anatomically different points)
2. **Marker names**: `r.ASIS_study`, `L.ASIS_study`, `r.PSIS_study`, `L.PSIS_study` from Pose2Sim augmentation
3. **TRC header fix**: Pose2Sim outputs TRC with header mismatch (22 in header, 65 in data) - handled automatically by `read_trc()`

## Code Style
- Follow PEP 8 with 4-space indentation, snake_case names
- Type-hint function signatures (see `main.py`, `data_stream.py`)
- Modules expose single public class (MediaStream, PoseDetector, VisualizeData)
- Concise docstrings for public methods; inline comments for non-obvious logic
- Raise clear exceptions on invalid input

## Commit Guidelines
- Imperative, scoped messages: `mediastream: validate fps metadata`
- Keep unrelated changes in separate commits
- PRs need summary, validation notes (commands run), issue refs, screenshots/GIFs for viz changes
- CI (pytest + lint) must pass
